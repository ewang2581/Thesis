{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('RawData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop patient ID and appointment ID variables\n",
    "cleaned = raw.drop(['PatientId', 'AppointmentID'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Date Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Scheduled Day and Appointment Day to datetime datatypes\n",
    "cleaned[\"ScheduledDay\"] = pd.to_datetime(cleaned[\"ScheduledDay\"])\n",
    "cleaned[\"AppointmentDay\"] = pd.to_datetime(cleaned[\"AppointmentDay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting day of the week\n",
    "cleaned[\"ScheduledDoW\"] = cleaned[\"ScheduledDay\"].dt.day_name()\n",
    "cleaned[\"AppointmentDoW\"] = cleaned[\"AppointmentDay\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature for days in between appointment day and scheduled day \n",
    "cleaned[\"DaysInBetween\"] = (cleaned[\"AppointmentDay\"] - cleaned[\"ScheduledDay\"].dt.normalize()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['ScheduledM'] = cleaned['ScheduledDay'].dt.month_name()\n",
    "cleaned['AppointmentM'] = cleaned['AppointmentDay'].dt.month_name()\n",
    "# Year is not included because all data points are from 2016\n",
    "# Quarter is not included because all data points are from appointments in months 4,5,6\n",
    "cleaned['AppointmentisWeekend'] = np.where(cleaned['AppointmentDoW'].isin(['Sunday', 'Saturday']), 1, 0)\n",
    "cleaned['ScheduledisWeekend'] = np.where(cleaned['ScheduledDoW'].isin(['Sunday', 'Saturday']), 1, 0)\n",
    "cleaned['ScheduledisPM'] = np.where(cleaned['ScheduledDay'].dt.hour < 12, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Data Entry Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any rows where age is negative \n",
    "cleaned = cleaned[cleaned.Age >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any rows where the scheduled date is after the appointment date \n",
    "cleaned = cleaned[cleaned['DaysInBetween'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.drop(['ScheduledDay', 'AppointmentDay'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NA in the entire data frame \n",
    "print(cleaned.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to perform One Hot Encoding on all the features listed in feature_to_encode\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]], drop_first = True)\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyclean = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_encode = ['Gender', 'Neighbourhood', 'ScheduledDoW', 'AppointmentDoW', 'ScheduledM', 'AppointmentM']\n",
    "\n",
    "for feature in features_to_encode:\n",
    "    copyclean = encode_and_bind(copyclean, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = copyclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move target variable to end of dataframe\n",
    "target = cleaned.pop(\"No-show\")\n",
    "cleaned.insert(cleaned.shape[1], \"No-show\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move gender variable to beginning\n",
    "gender = cleaned.pop(\"Gender_M\")\n",
    "cleaned.insert(0, \"Gender_M\", gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature selection \n",
    "ind = [1, 2, 3, 4, 7, 8, 11, 44, 47, 50, 51, 72, 73, 77, 81, 105, 106, 107, 109, 110]\n",
    "\n",
    "for i in ind:\n",
    "    print(cleaned.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into feature variables (X) and target variable (y)\n",
    "X = cleaned.iloc[:, :-1].values\n",
    "y = cleaned.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization (mean removal and variance scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as NPY files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_scaled', X_train_scaled)\n",
    "np.save('y_train', y_train)\n",
    "np.save('X_test_scaled', X_test_scaled)\n",
    "np.save('y_test', y_test)\n",
    "\n",
    "np.save('X_train', X_train)\n",
    "np.save('X_test', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = cleaned.shape[0]\n",
    "print(\"Total number of data samples: \", total_rows)\n",
    "# Percentage of patients that are male\n",
    "male = cleaned.apply(lambda x: True if x['Gender_M'] == 1 else False, axis = 1)\n",
    "m_rows = len(male[male == True].index)\n",
    "print(\"Male: \", m_rows/total_rows)\n",
    "# Percentage of patients that are female\n",
    "female = cleaned.apply(lambda x: True if x['Gender_M'] == 0 else False, axis = 1)\n",
    "f_rows = len(female[female == True].index)\n",
    "print(\"Female: \", f_rows/total_rows)\n",
    "# Percentage of patients that have scholarship \n",
    "scholarship = cleaned.apply(lambda x: True if x['Scholarship'] == 1 else False, axis = 1)\n",
    "s_rows = len(scholarship[scholarship == True].index)\n",
    "print(\"Scholarship: \", s_rows/total_rows)\n",
    "# Percentage of patients with no scholarship\n",
    "noscholarship = cleaned.apply(lambda x: True if x['Scholarship'] == 0 else False, axis = 1)\n",
    "ns_rows = len(noscholarship[noscholarship == True].index)\n",
    "print(\"No scholarship: \", ns_rows/total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(no show | male)\n",
    "male_noshow = cleaned.apply(lambda x: True if (x['Gender_M'] == 1 and x['No-show'] == \"Yes\") else False, axis = 1)\n",
    "m_noshow_rows = len(male_noshow[male_noshow == True].index)\n",
    "print(\"Male: \", m_noshow_rows/m_rows)\n",
    "# P(no show | female)\n",
    "female_noshow = cleaned.apply(lambda x: True if (x['Gender_M'] == 0 and x['No-show'] == \"Yes\") else False, axis = 1)\n",
    "f_noshow_rows = len(female_noshow[female_noshow == True].index)\n",
    "print(\"Female: \", f_noshow_rows/f_rows)\n",
    "# P(no show | scholarship)\n",
    "scholarship_noshow = cleaned.apply(lambda x: True if x['Scholarship'] == 1 and x['No-show'] == \"Yes\" else False, axis = 1)\n",
    "s_noshow_rows = len(scholarship_noshow[scholarship_noshow == True].index)\n",
    "print(\"Scholarship: \", s_noshow_rows/s_rows)\n",
    "# P(no show | no scholarship) \n",
    "noscholarship_noshow = cleaned.apply(lambda x: True if x['Scholarship'] == 0 and x['No-show'] == \"Yes\" else False, axis = 1)\n",
    "ns_noshow_rows = len(noscholarship_noshow[noscholarship_noshow == True].index)\n",
    "print(\"No scholarship: \", ns_noshow_rows/ns_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the no-shows...\n",
    "noshow = cleaned.apply(lambda x: True if x['No-show'] == \"Yes\" else False, axis = 1)\n",
    "total_noshow = len(noshow[noshow == True].index)\n",
    "print(\"Total no shows: \", total_noshow)\n",
    "print(\"Proportion of no shows: \", total_noshow / total_rows)\n",
    "# P(male | no show)\n",
    "print(\"Male: \", m_noshow_rows / total_noshow)\n",
    "# P(female | no show)\n",
    "print(\"Female: \", f_noshow_rows / total_noshow)\n",
    "# P(scholarship | no show)\n",
    "print(\"Scholarship: \", s_noshow_rows / total_noshow)\n",
    "# P(no scholarship | no show)\n",
    "print(\"No scholarship: \", ns_noshow_rows / total_noshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data File for TOF-1\n",
    "This dataframe will eliminate socioeconomic factors such as neighborhood and scholarship. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_delete = []\n",
    "for i in range(len(colname)):\n",
    "    if(\"Scholarship\" in colname[i] or \"Neighbourhood\" in colname[i]):\n",
    "        col_to_delete.append(colname[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.drop(labels = col_to_delete, axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into feature variables (X) and target variable (y)\n",
    "TOF1_X = cleaned.iloc[:, :-1].values\n",
    "TOF1_y = cleaned.iloc[:, -1].values\n",
    "\n",
    "labelencoder_y = LabelEncoder()\n",
    "TOF1_y = labelencoder_y.fit_transform(TOF1_y)\n",
    "\n",
    "TOF1_X_train, TOF1_X_test, TOF1_y_train, TOF1_y_test = train_test_split(TOF1_X, TOF1_y, test_size = 0.2)\n",
    "\n",
    "#scaler = sklearn.preprocessing.StandardScaler().fit(TOF1_X_train)\n",
    "#TOF1_X_train_scaled = scaler.transform(TOF1_X_train)\n",
    "#TOF1_X_test_scaled = scaler.transform(TOF1_X_test)\n",
    "\n",
    "np.save('TOF1_X_train', TOF1_X_train)\n",
    "np.save('TOF1_X_test', TOF1_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Unbalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NS-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full dataset \n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.5)\n",
    "under = RandomUnderSampler(sampling_strategy=0.75)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "SMOTE_X_train, SMOTE_y_train = pipeline.fit_resample(X_train, y_train)\n",
    "#SMOTE_X_train, SMOTE_X_test, SMOTE_y_train, SMOTE_y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# for TOF-1 dataset \n",
    "SMOTE_TOF1_X_train, SMOTE_TOF1_y_train = pipeline.fit_resample(TOF1_X_train, TOF1_y_train)\n",
    "#SMOTE_TOF1_X_train, SMOTE_TOF1_X_test, SMOTE_TOF1_y_train, SMOTE_TOF1_y_test = train_test_split(SMOTE_TOF1_X, SMOTE_TOF1_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets \n",
    "np.save('ns_SMOTE_X_train', SMOTE_X_train)\n",
    "\n",
    "np.save('ns_SMOTE_y_train', SMOTE_y_train)\n",
    "\n",
    "np.save('ns_SMOTE_TOF1_X_train', SMOTE_TOF1_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SS-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np.concatenate((X_train, np.expand_dims(y_train, axis = 1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = total[:, 2]\n",
    "X_train_del = np.delete(total, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=1)\n",
    "under = RandomUnderSampler(sampling_strategy=1)\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "SMOTE_X_train_del, SMOTE_sc = pipeline.fit_resample(X_train_del, sc)\n",
    "\n",
    "counter = Counter(SMOTE_sc)\n",
    "print(counter)\n",
    "\n",
    "counter = Counter(SMOTE_sc)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the column back into the dataset \n",
    "SMOTE_dataset = np.insert(SMOTE_X_train_del, 2, SMOTE_sc, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into feature variables (X) and target variable (y)\n",
    "s_SMOTE_X_train = SMOTE_dataset[:, :-1]\n",
    "s_SMOTE_y_train = SMOTE_dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets \n",
    "np.save('scholarship_SMOTE_X_train', s_SMOTE_X_train)\n",
    "np.save('scholarship_SMOTE_y_train', s_SMOTE_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
