{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in all datafiles\n",
    "X_train_scaled = np.load('X_train_scaled.npy')\n",
    "X_test_scaled = np.load('X_test_scaled.npy')\n",
    "\n",
    "X_train_original = np.load('X_train.npy')\n",
    "y_train_original = np.load('y_train.npy')\n",
    "\n",
    "# Data from SMOTE \n",
    "ns_SMOTE_X_train = np.load('ns_SMOTE_X_train.npy')\n",
    "ns_SMOTE_y_train = np.load('ns_SMOTE_y_train.npy')\n",
    "\n",
    "scholarship_SMOTE_X_train = np.load('scholarship_SMOTE_X_train.npy')\n",
    "scholarship_SMOTE_y_train = np.load('scholarship_SMOTE_y_train.npy')\n",
    "\n",
    "# Data from IHT\n",
    "ns_IHT_X_train = np.load('ns_IHT_X_train.npy')\n",
    "ns_IHT_y_train = np.load('ns_IHT_y_train.npy')\n",
    "\n",
    "scholarship_IHT_X_train = np.load('scholarship_IHT_X_train.npy')\n",
    "scholarship_IHT_y_train = np.load('scholarship_IHT_y_train.npy')\n",
    "\n",
    "# Testing: (hasn't been changed)\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "# Raw: used for grouping \n",
    "X_test_raw = np.load('X_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: change based on which set of data you want to run it on\n",
    "X_train = ns_SMOTE_X_train\n",
    "y_train = ns_SMOTE_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization \n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholarship_index = np.where(X_test_raw[:, 2] == 1)\n",
    "\n",
    "# Divide into separate testing sets \n",
    "y_test_scholarship = np.squeeze(np.take(y_test, scholarship_index))\n",
    "y_test_noscholarship = np.delete(y_test, scholarship_index)\n",
    "y_test_list = [y_test, y_test_scholarship, y_test_noscholarship]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that no data points were lost\n",
    "print(len(y_test_scholarship) + len(y_test_noscholarship))\n",
    "print(len(y_test))\n",
    "print(len(y_test_scholarship))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_curve, confusion_matrix, roc_curve, roc_auc_score, brier_score_loss, f1_score, auc\n",
    "evaluation_stats = {}\n",
    "ROC = []\n",
    "PRCurve = []\n",
    "\n",
    "# Helper function for displaying model evaluation metrics\n",
    "def evaluate(y_pred, probs, model_name, print_toggle):\n",
    "    # Overall ROC Curve \n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    ROC.append({'label': model_name,\n",
    "               'FPR': fpr,\n",
    "               'TPR': tpr})\n",
    "    \n",
    "    # Precision-recall curve \n",
    "    p, r, _ = precision_recall_curve(y_test, probs)\n",
    "    PRCurve.append({'label': model_name,\n",
    "                   'recall': r, \n",
    "                   'precision': p})\n",
    "    \n",
    "    # split predictions into categories \n",
    "    y_pred_scholarship = np.squeeze(np.take(y_pred, scholarship_index))\n",
    "    y_pred_noscholarship = np.delete(y_pred, scholarship_index)\n",
    "    y_pred_list = [y_pred, y_pred_scholarship, y_pred_noscholarship]\n",
    "\n",
    "    probs_scholarship = np.squeeze(np.take(probs, scholarship_index))\n",
    "    probs_noscholarship = np.delete(probs, scholarship_index)\n",
    "    probs_list = [probs, probs_scholarship, probs_noscholarship]\n",
    "    \n",
    "    # Organization of evaluation stats: \n",
    "\n",
    "    group_names = [\"Overall\", \"Scholarship\", \"No Scholarship\"]\n",
    "    evaluation_stats[model_name] = {}\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize = (20, 5), sharey=True)\n",
    "    for i in range(len(y_pred_list)):\n",
    "        # convert from Boolean to 0 and 1 \n",
    "        if (y_pred_list[i].dtype == bool):\n",
    "            y_pred_list[i] = [1 if p == True else 0 for p in y_pred_list[i]]\n",
    "        \n",
    "        group = []\n",
    "        \n",
    "        # accuracy\n",
    "        accuracy = accuracy_score(y_test_list[i], y_pred_list[i])\n",
    "        group.append(accuracy)\n",
    "        \n",
    "        # recall\n",
    "        recall = recall_score(y_test_list[i], y_pred_list[i])\n",
    "        group.append(recall)\n",
    "        \n",
    "        # precision \n",
    "        precision = precision_score(y_test_list[i], y_pred_list[i])\n",
    "        group.append(precision)\n",
    "        \n",
    "        # f1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        group.append(f1)\n",
    "        \n",
    "        # false negative and false positive \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_list[i], y_pred_list[i]).ravel()\n",
    "        fnr = fn / (fn + tp)\n",
    "        group.append(fnr)\n",
    "        fpr = fp / (fp + tn)\n",
    "        group.append(fpr)\n",
    "        \n",
    "        # Group Confusion Matrix \n",
    "        cf_matrix = confusion_matrix(y_test_list[i], y_pred_list[i])\n",
    "\n",
    "        l = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        l_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                        cf_matrix.flatten()]\n",
    "        l_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                             cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "                  zip(l,l_counts,l_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "        if(group_names[i] == \"Overall\"):\n",
    "            sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', ax = axes[i])\n",
    "        else:\n",
    "            sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='YlOrBr', ax = axes[i])\n",
    "        axes[i].set_title(group_names[i] + \" Confusion Matrix\")\n",
    "        axes[i].set_ylim([0,2])\n",
    "        \n",
    "        # AUC \n",
    "        auc = roc_auc_score(y_test_list[i], probs_list[i])\n",
    "        group.append(auc)\n",
    "        \n",
    "        # Brier score \n",
    "        b_score = brier_score_loss(y_test_list[i], probs_list[i])\n",
    "        group.append(b_score)\n",
    "        \n",
    "        evaluation_stats[model_name][group_names[i]] = group\n",
    "        if(print_toggle):\n",
    "            print(group_names[i])\n",
    "            print(\"Accuracy: \\t\", accuracy)\n",
    "            print(\"Recall: \\t\", recall)\n",
    "            print(\"Precision: \\t\", precision)\n",
    "            print(\"F1: \\t\", f1)\n",
    "            print(\"False Negative Rate: \\t\", fnr)\n",
    "            print(\"False Positive Rate: \\t\", fpr)\n",
    "            print(\"AUC: \\t\", auc)\n",
    "            print(\"Brier Score: \\t\", b_score)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression \n",
    "\n",
    "selector = SelectKBest(f_regression, k = 20)\n",
    "X_train_reg = selector.fit_transform(X_train, y_train)\n",
    "cols = selector.get_support(indices = True)\n",
    "print(cols)\n",
    "X_test_reg = selector.fit_transform(X_test, y_test)\n",
    "\n",
    "# for classification \n",
    "from sklearn.feature_selection import f_classif\n",
    "selector = SelectKBest(f_classif, k = 20)\n",
    "X_train_class = selector.fit_transform(X_train, y_train)\n",
    "cols = selector.get_support(indices = True)\n",
    "print(cols)\n",
    "X_test_class = selector.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "Predicting everyone is a show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(y_test))\n",
    "y_cal = np.zeros(len(y_test))\n",
    "evaluate(y_pred, y_cal, \"Base\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "distributions = dict(fit_intercept=[True], normalize=[True])\n",
    "search = GridSearchCV(reg, distributions).fit(X_train_reg, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_cal = search.predict(X_test_reg)\n",
    "scaler = MinMaxScaler()\n",
    "y_cal = scaler.fit_transform(y_cal.reshape(-1, 1))\n",
    "\n",
    "# due to Python rounding error\n",
    "for i in range(len(y_cal)):\n",
    "    if y_cal[i] > 1:\n",
    "        y_cal[i] = 1\n",
    "\n",
    "y_pred = y_cal > 0.5\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"OLS\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.ElasticNet()\n",
    "\n",
    "grid = dict()\n",
    "grid['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "grid['l1_ratio'] = np.arange(0, 1, 0.01)\n",
    "search = RandomizedSearchCV(reg, grid).fit(X_train_reg, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_cal = search.predict(X_test_reg)\n",
    "scaler = MinMaxScaler()\n",
    "y_cal = scaler.fit_transform(y_cal.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# due to Python rounding error\n",
    "for i in range(len(y_cal)):\n",
    "    if y_cal[i] > 1:\n",
    "        y_cal[i] = 1\n",
    "        \n",
    "y_pred = y_cal > 0.5\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"Elastic Net\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.LogisticRegression()\n",
    "grid = dict()\n",
    "grid['C'] = np.arange(0, 100, 0.01)\n",
    "search = RandomizedSearchCV(clf, grid).fit(X_train_reg, y_train)\n",
    "\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_reg)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_reg)[:, 1]\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_reg, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_reg)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"Logistic\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability = True, kernel = 'rbf', max_iter = 10)\n",
    "grid = dict()\n",
    "grid['gamma'] = np.arange(0, 10, 0.01)\n",
    "grid['C'] = np.arange(0, 100, 0.01)\n",
    "#grid['max_iter'] = [20]\n",
    "\n",
    "search = RandomizedSearchCV(svm, grid).fit(X_train_class, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_class)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_class)[:, 1]\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_class, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_class)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"SVM\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors = 8, p = 1)\n",
    "#grid = dict()\n",
    "#grid['n_neighbors'] = np.arange(1, 25, 1)\n",
    "#grid['p'] = [1, 2, 3]\n",
    "\n",
    "search = neigh.fit(X_train_class, y_train)\n",
    "#print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_class)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_class)\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_class, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_class)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"kNN\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "grid = dict()\n",
    "grid['criterion'] = [\"gini\"]\n",
    "grid['max_depth'] = np.arange(5, 10, 1)\n",
    "grid['min_samples_leaf'] = np.arange(2, 10, 1)\n",
    "grid['min_samples_split'] = np.arange(2, 10, 1)\n",
    "\n",
    "search = RandomizedSearchCV(dt, grid).fit(X_train_class, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_class)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_class)\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_class, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_class)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"Decision Trees\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_features = \"auto\", criterion = \"entropy\")\n",
    "grid = dict()\n",
    "grid['n_estimators'] = np.arange(2, 20, 2)\n",
    "grid['max_depth'] = np.arange(5, 50, 5)\n",
    "grid['min_samples_leaf'] = np.arange(2, 10, 1)\n",
    "grid['min_samples_split'] = np.arange(2, 10, 1)\n",
    "\n",
    "search = RandomizedSearchCV(rf, grid).fit(X_train_class, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_class)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_class)\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_class, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_class)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"Random Forest\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(activation = \"logistic\", early_stopping = True, validation_fraction = 0.1, learning_rate = \"invscaling\", max_iter = 100)\n",
    "grid = dict()\n",
    "grid['hidden_layer_sizes'] = [(25, ), (50, ), (75, ), (100, )]\n",
    "grid['alpha'] = np.arange(0, 10, 0.01)\n",
    "grid['batch_size'] = np.arange(6, 512, 8)\n",
    "\n",
    "search = RandomizedSearchCV(nn, grid).fit(X_train_class, y_train)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = search.predict(X_test_class)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncalibrated \n",
    "y_uncal = search.predict_proba(X_test_class)\n",
    "# calibrated \n",
    "calibrator = CalibratedClassifierCV(search, cv = 3)\n",
    "calibrator.fit(X_test_class, y_test)\n",
    "y_cal = calibrator.predict_proba(X_test_class)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred, y_cal, \"MLP\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Evaluation Metrics to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(evaluation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ns_SMOTE_evaluation_stats.txt\",\"w\")\n",
    "f.write( str(evaluation_stats) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ROC:\n",
    "    plt.plot(x['FPR'], x['TPR'], label = x['label'])\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"Specificity (FPR)\")\n",
    "plt.ylabel(\"Sensitivity (TPR)\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in PRCurve:\n",
    "    plt.plot(x['recall'], x['precision'], label = x['label'])\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
